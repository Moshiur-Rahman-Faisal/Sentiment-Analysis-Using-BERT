{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-6hmel6sKNSP"
   },
   "outputs": [],
   "source": [
    "def reproduceResult():\n",
    "  seed_value= 0\n",
    "\n",
    "  \n",
    "  with tf.device(\"/cpu:0\"):\n",
    "    ...\n",
    "\n",
    "\n",
    "  os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "  np.random.seed(0)\n",
    "  rn.seed(0)\n",
    "\n",
    "\n",
    "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, \n",
    "                                          inter_op_parallelism_threads=1)\n",
    "\n",
    "\n",
    "  tf.compat.v1.set_random_seed(seed_value)\n",
    "  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "  tf.compat.v1.keras.backend.clear_session()\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vURLkAC5_Jp0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_17088/4179402048.py:20: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_17088/628588012.py:43: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "  \n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from tensorflow import keras\n",
    "\n",
    "reproduceResult()\n",
    "# %tensorflow_version 2.x\n",
    "# import tensorflow as tf\n",
    "# tf.test.gpu_device_name()\n",
    "# from scipy import integrate\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from tensorflow import keras\n",
    "import tempfile\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "# import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from keras_lr_finder import LRFinder\n",
    "from clr.clr_callback import CyclicLR\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "\n",
    "\n",
    "import keras_tuner\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moshi\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\moshi\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('data.csv',index_col=False, encoding='iso-8859-1', \n",
    "                                warn_bad_lines=True, error_bad_lines=False)\n",
    "\n",
    "df2 = pd.read_csv('data.csv',index_col=False, encoding='iso-8859-1', \n",
    "                                warn_bad_lines=True, error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_17088/3898086040.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1.label[df1.label == 1]=\"acpt\"\n",
      "C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_17088/3898086040.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2.label[df2.label == 1]=\"acpt\"\n"
     ]
    }
   ],
   "source": [
    "df1.label[df1.label == 1]=\"acpt\"\n",
    "df1.label[df1.label == 0]=\"unac\"\n",
    "\n",
    "df2.label[df2.label == 1]=\"acpt\"\n",
    "df2.label[df2.label == 0]=\"unac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acpt    6023\n",
       "unac    2528\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acpt    6023\n",
       "unac    2528\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking acceptable and deleting unacceptable\n",
    "df3 = df1[~df1.label.str.contains('acpt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      label                                               text\n",
       "18    unac                                They drank the pub.\n",
       "20    unac                           The professor talked us.\n",
       "22    unac                               We yelled ourselves.\n",
       "23    unac                            We yelled Harry hoarse.\n",
       "25    unac                             Harry coughed himself.\n",
       "...    ...                                                ...\n",
       "8531  unac                        Anson believed to be happy.\n",
       "8539  unac               Anson left before Jenny saw himself.\n",
       "8545  unac  Anson thought that himself was going to the club.\n",
       "8546  unac                   Poseidon appears to own a dragon\n",
       "8547  unac                     Digitize is my happiest memory\n",
       "\n",
       "[2528 rows x 2 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly picking 250 rows\n",
    "df3 = df3.sample(n=250, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unac    250\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking unacceptable and deleting unacceptable\n",
    "df4 = df2[~df2.label.str.contains(\"unac\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly picking 250 rows\n",
    "df4 = df4.sample(n=250, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acpt    250\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concating acceptable and unacceptable\n",
    "frames = [df3, df4]\n",
    "\n",
    "temp = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unac    250\n",
       "acpt    250\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFgD7Seo_Xlq",
    "outputId": "9be2fe5a-b7b5-4488-9cd4-8a48ce6123e8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 1069\n",
      "[[   0    0    0 ...  204  377  205]\n",
      " [   0    0    0 ...  141   25   97]\n",
      " [   0    0    0 ...    7    2  206]\n",
      " ...\n",
      " [   0    0    0 ...   84  203  375]\n",
      " [   0    0    0 ...  164   35 1068]\n",
      " [   0    0    0 ...    2    1  109]]\n",
      "(1070, 300)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(temp, test_size=0.2, stratify = temp['label'], random_state = 42)\n",
    "num_classes = 2\n",
    "embed_num_dims = 300\n",
    "max_seq_len = 50\n",
    "\n",
    "x_train = train['text']\n",
    "x_test = test['text']\n",
    "\n",
    "y_train = train['label']\n",
    "y_test = test['label']\n",
    "\n",
    "texts_train = x_train\n",
    "texts_test = x_test\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "\n",
    "vocab_size = len(index_of_words) + 1\n",
    "\n",
    "print('Number of unique words: {}'.format(len(index_of_words)))\n",
    "\n",
    "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len, padding='pre' )\n",
    "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len,  padding='pre')\n",
    "\n",
    "print(X_train_pad)\n",
    "\n",
    "\n",
    "encoding = {\n",
    "    'acpt': 1,\n",
    "    'unac': 0\n",
    "}\n",
    "\n",
    "y_train = [encoding[x] for x in train['label']]\n",
    "y_test = [encoding[x] for x in test['label']]\n",
    "\n",
    "\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "def create_embedding_matrix(word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    with open('C:/Users/moshi/Python Code/Untitled Folder/cc.en.300.vec',encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
    "print(embedd_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGgsd5mMZPKn"
   },
   "source": [
    "# Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n",
    "\n",
    "def get_flops(model):\n",
    "    concrete = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete.get_concrete_function(\n",
    "        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
    "        return flops.total_float_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IvOZoK8YGDI",
    "outputId": "fe861031-a89e-45d3-8f7a-42f1e7b6b256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 Complete [00h 00m 08s]\n",
      "val_accuracy: 0.5299999713897705\n",
      "\n",
      "Best val_accuracy So Far: 0.6399999856948853\n",
      "Total elapsed time: 00h 08m 15s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in 1652970952\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 96\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.30000000000000004\n",
      "lstm_unit: 224\n",
      "lstm_dropout: 0.2\n",
      "cnn_2_unit: 128\n",
      "cnn_2_dropout: 0.4\n",
      "Score: 0.6399999856948853\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 128\n",
      "cnn_1_unit: 32\n",
      "cnn_1_dropout: 0.30000000000000004\n",
      "lstm_unit: 64\n",
      "lstm_dropout: 0.30000000000000004\n",
      "cnn_2_unit: 192\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.6399999856948853\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 48\n",
      "cnn_1_unit: 48\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 160\n",
      "lstm_dropout: 0.1\n",
      "cnn_2_unit: 224\n",
      "cnn_2_dropout: 0.5\n",
      "Score: 0.5899999737739563\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 80\n",
      "cnn_1_unit: 64\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 160\n",
      "lstm_dropout: 0.1\n",
      "cnn_2_unit: 224\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.5799999833106995\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 64\n",
      "cnn_1_unit: 64\n",
      "cnn_1_dropout: 0.1\n",
      "lstm_unit: 192\n",
      "lstm_dropout: 0.30000000000000004\n",
      "cnn_2_unit: 192\n",
      "cnn_2_dropout: 0.2\n",
      "Score: 0.5799999833106995\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 48\n",
      "cnn_1_unit: 64\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 96\n",
      "lstm_dropout: 0.30000000000000004\n",
      "cnn_2_unit: 128\n",
      "cnn_2_dropout: 0.30000000000000004\n",
      "Score: 0.5600000023841858\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 96\n",
      "cnn_1_unit: 48\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 128\n",
      "lstm_dropout: 0.30000000000000004\n",
      "cnn_2_unit: 160\n",
      "cnn_2_dropout: 0.2\n",
      "Score: 0.550000011920929\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 112\n",
      "cnn_1_unit: 32\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 192\n",
      "lstm_dropout: 0.4\n",
      "cnn_2_unit: 192\n",
      "cnn_2_dropout: 0.5\n",
      "Score: 0.550000011920929\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 80\n",
      "cnn_1_unit: 80\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 128\n",
      "lstm_dropout: 0.2\n",
      "cnn_2_unit: 192\n",
      "cnn_2_dropout: 0.2\n",
      "Score: 0.5400000214576721\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "attention_unit: 112\n",
      "cnn_1_unit: 48\n",
      "cnn_1_dropout: 0.2\n",
      "lstm_unit: 96\n",
      "lstm_dropout: 0.5\n",
      "cnn_2_unit: 224\n",
      "cnn_2_dropout: 0.2\n",
      "Score: 0.5400000214576721\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "import time\n",
    "LOG_DIR = f\"{int(time.time())}\"\n",
    "seed_value= 0\n",
    "\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "  \n",
    "  reproduceResult()\n",
    "\n",
    "  print('Ya it comes here')\n",
    "  unit_attention = hp.Int(\"attention_unit\",min_value =32, max_value = 128, step = 16)\n",
    "  fake_val = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
    "  cnn_1_unit = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
    "  cnn_1_dropout = hp.Float(\"cnn_1_dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
    "\n",
    "  lstm_unit = hp.Int(\"lstm_unit\",min_value =64, max_value = 256, step = 32)\n",
    "  lstm_dropout = hp.Float(\"lstm_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
    "  cnn_2_unit = hp.Int(\"cnn_2_unit\",min_value =64, max_value = 256, step = 32)\n",
    "  cnn_2_dropout = hp.Float(\"cnn_2_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
    "\n",
    "\n",
    "\n",
    "  seq_input = keras.layers.Input(shape=(max_seq_len,))\n",
    "\n",
    "  embedded = keras.layers.Embedding(vocab_size,\n",
    "                          embed_num_dims,\n",
    "                          input_length = max_seq_len,\n",
    "                          weights = [embedd_matrix])(seq_input)\n",
    "\n",
    "  cnn = keras.layers.Conv1D(cnn_1_unit,3,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                            bias_regularizer=regularizers.l2(1e-2),\n",
    "                            activity_regularizer=regularizers.l2(1e-4))(embedded)\n",
    "  cnn = keras.layers.Activation(activation='relu')(cnn)\n",
    "  cnn = keras.layers.BatchNormalization()(cnn)\n",
    "  cnn = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn)\n",
    "\n",
    "\n",
    "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(lstm_unit, recurrent_regularizer=regularizers.l2(1e-4),\n",
    "                                                      return_sequences=True,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                                                      bias_regularizer=regularizers.l2(1e-2),\n",
    "                                                      activity_regularizer=regularizers.l2(1e-4),input_shape =(48,)))(cnn)\n",
    "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
    "  lstm = keras.layers.BatchNormalization()(lstm)\n",
    "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
    "  \n",
    "  \n",
    "\n",
    "  cnn_2 = keras.layers.Conv1D(cnn_2_unit,3,kernel_regularizer=regularizers.l2(1e-4),\n",
    "                            bias_regularizer=regularizers.l2(1e-2),\n",
    "                            activity_regularizer=regularizers.l2(1e-4))(lstm)\n",
    "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
    "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
    "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
    "\n",
    "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
    "  output = keras.layers.Dense(num_classes, activation='softmax')(max_pooling)\n",
    "\n",
    "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
    "  model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "  model.summary()\n",
    "  \n",
    "  print(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\n",
    "    \n",
    "  print(\" \")\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "clr_step_size = int((len(X_train_pad)/64))\n",
    "base_lr = 1e-3\n",
    "max_lr = 6e-3\n",
    "mode = 'exp_range'\n",
    "\n",
    "\n",
    "clr = CyclicLR(base_lr = base_lr, max_lr = max_lr, step_size = clr_step_size, mode = mode)\n",
    "\n",
    "stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                              patience=4,\n",
    "                              restore_best_weights=True,\n",
    "                              verbose=0, mode='max')\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
    "    max_trials = 40,\n",
    "    executions_per_trial = 1,\n",
    "    directory = LOG_DIR\n",
    "    )\n",
    "  \n",
    "tuner.search(x=X_train_pad,y = y_train,epochs = 20, batch_size = 64,callbacks = [stop,clr], \n",
    "             validation_data = (X_test_pad,y_test))\n",
    "\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 7\n",
      "attention_unit (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 16, 'sampling': None}\n",
      "cnn_1_unit (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 96, 'step': 16, 'sampling': None}\n",
      "cnn_1_dropout (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.3, 'step': 0.1, 'sampling': None}\n",
      "lstm_unit (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "lstm_dropout (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': None}\n",
      "cnn_2_unit (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "cnn_2_dropout (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ya it comes here\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 300)           321000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 48, 80)            72080     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 48, 80)            0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 80)           320       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 48, 80)            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 48, 448)          546560    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 48, 448)           0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 48, 448)          1792      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 48, 448)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 46, 128)           172160    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 46, 128)           0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 46, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 46, 128)           0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,114,682\n",
      "Trainable params: 1,113,370\n",
      "Non-trainable params: 1,312\n",
      "_________________________________________________________________\n",
      "The FLOPs is:8023843\n",
      " \n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 300)           321000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 48, 80)            72080     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 48, 80)            0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 80)           320       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 48, 80)            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 48, 448)          546560    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 48, 448)           0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 48, 448)          1792      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 48, 448)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 46, 128)           172160    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 46, 128)           0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 46, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 46, 128)           0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,114,682\n",
      "Trainable params: 1,113,370\n",
      "Non-trainable params: 1,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models = tuner.get_best_models(num_models=1)\n",
    "best_model = models[0]\n",
    "# Build the model.\n",
    "# Needed for `Sequential` without specified `input_shape`.\n",
    "best_model.build(input_shape=(None, 28, 28))\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ya it comes here\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 300)           321000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 48, 80)            72080     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 48, 80)            0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 80)           320       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 48, 80)            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 48, 448)          546560    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 48, 448)           0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 48, 448)          1792      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 48, 448)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 46, 128)           172160    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 46, 128)           0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 46, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 46, 128)           0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,114,682\n",
      "Trainable params: 1,113,370\n",
      "Non-trainable params: 1,312\n",
      "_________________________________________________________________\n",
      "The FLOPs is:8023843\n",
      " \n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 5s 316ms/step - loss: 7.4768 - accuracy: 0.4950 - val_loss: 5.0927 - val_accuracy: 0.5800\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 2s 216ms/step - loss: 7.1452 - accuracy: 0.5800 - val_loss: 5.0240 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 2s 247ms/step - loss: 6.5996 - accuracy: 0.6250 - val_loss: 4.9182 - val_accuracy: 0.4900\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 2s 367ms/step - loss: 6.0871 - accuracy: 0.7350 - val_loss: 4.7998 - val_accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 5.6788 - accuracy: 0.8025 - val_loss: 4.4993 - val_accuracy: 0.6000\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 5.2447 - accuracy: 0.8850 - val_loss: 4.3220 - val_accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 2s 282ms/step - loss: 4.6285 - accuracy: 0.9275 - val_loss: 4.1567 - val_accuracy: 0.5000\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 2s 285ms/step - loss: 4.2792 - accuracy: 0.9425 - val_loss: 3.9273 - val_accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 2s 275ms/step - loss: 4.0324 - accuracy: 0.9425 - val_loss: 3.7933 - val_accuracy: 0.6400\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 2s 289ms/step - loss: 3.7860 - accuracy: 0.9400 - val_loss: 3.6134 - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 2s 283ms/step - loss: 3.6828 - accuracy: 0.9550 - val_loss: 3.5085 - val_accuracy: 0.5100\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 2s 280ms/step - loss: 3.5271 - accuracy: 0.9400 - val_loss: 3.3279 - val_accuracy: 0.5200\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 2s 280ms/step - loss: 3.3159 - accuracy: 0.9550 - val_loss: 3.1782 - val_accuracy: 0.5900\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(5)\n",
    "# Build the model with the best hp.\n",
    "model = build_model(best_hps[0])\n",
    "# Fit with the entire dataset.\n",
    "history = model.fit(x=X_train_pad,y = y_train,epochs = 20, batch_size = 64,callbacks = [stop,clr], \n",
    "             validation_data = (X_test_pad,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 8s 438ms/step - loss: 3.7520 - accuracy: 0.9650 - val_loss: 3.6702 - val_accuracy: 0.5300\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 2s 267ms/step - loss: 3.4910 - accuracy: 0.9700 - val_loss: 3.5038 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 2s 267ms/step - loss: 3.2693 - accuracy: 0.9775 - val_loss: 3.4103 - val_accuracy: 0.4900\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 2s 263ms/step - loss: 3.0904 - accuracy: 0.9825 - val_loss: 3.2232 - val_accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 2s 262ms/step - loss: 2.9465 - accuracy: 0.9850 - val_loss: 3.1484 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = best_model.fit(x=X_train_pad,y = y_train,epochs = 20, batch_size = 64,callbacks = [stop,clr], \n",
    "             validation_data = (X_test_pad,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8875\n",
      "Testing Accuracy:  0.6400\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_pad, y_train, verbose=0)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_17088/819814676.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\moshi\\AppData\\Local\\Temp/ipykernel_17088/819814676.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    https://keras.io/guides/keras_tuner/getting_started/\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://keras.io/guides/keras_tuner/getting_started/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "100_percent_test_BiLSTM_best_model_git.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
